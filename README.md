
#  CodeExplainer â€“ Streaming LLM Responses in Jupyter

### This project demonstrates how to stream responses from Large Language Models (LLMs) such as OpenAI GPT-4o-mini and LLaMA (via Ollama) inside a Jupyter Notebook with real-time display updates.

### It is an interactive AI tool that can analyze, understand, and explain source code. Users simply pass their question or code into the question variable, and the system generates a live explanation.
---

##  Features

- **Live streaming** token-by-token  
-  Real-time rendering in Jupyter using `update_display`   
-  Supports:
  - OpenAI models (`gpt-4o-mini`)
  - Local LLaMA models via **Ollama**
